{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from geopy.geocoders import Nominatim\n",
    "from elasticsearch import Elasticsearch\n",
    "import spacy\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Elasticsearch and mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    [{'host': 'localhost', 'port': 9200, 'scheme': 'http'}],http_auth=('osama', 'osama123'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_mapping = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"title_analyzer\",\n",
    "                \"search_analyzer\": \"standard\"\n",
    "            },\n",
    "            'content': {\n",
    "                'type': 'text',\n",
    "                'analyzer': 'content_analyzer'\n",
    "            },\n",
    "            \"authors\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"first_name\": {\n",
    "                        \"type\": \"text\"\n",
    "                    },\n",
    "                    \"last_name\": {\n",
    "                        \"type\": \"text\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"date\": {\n",
    "                \"type\": \"date\"\n",
    "            },\n",
    "            \"geopoint\": {\n",
    "                \"type\": \"geo_point\"\n",
    "            },\n",
    "            \"temporal_expressions\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"georeferences\": {\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"title_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"autocomplete_tokenizer\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\"\n",
    "                    ]\n",
    "                },\n",
    "                \"content_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"char_filter\": [\n",
    "                        \"html_strip\"\n",
    "                    ],\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"stop\",\n",
    "                        \"length\",\n",
    "                        \"porter_stem\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"autocomplete_tokenizer\": {\n",
    "                    \"type\": \"edge_ngram\",\n",
    "                    \"min_gram\": 2,\n",
    "                    \"max_gram\": 10,\n",
    "                    \"token_chars\": [\n",
    "                        \"letter\",\n",
    "                        \"digit\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"reuter_news_index\"\n",
    "es.indices.create(index=index_name, body=index_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extracting for all fields\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use the `extract date` function to extract dates from data and make sure it is applicable with Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(reuters_tag):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - reuters_tag: A BeautifulSoup Tag object representing a 'reuters' element with the date information.\n",
    "\n",
    "    Return:\n",
    "    - A formatted date string in Elasticsearch-friendly format if the 'date' tag is present; otherwise, returns None.\n",
    "    \"\"\"\n",
    "    date_tag = reuters_tag.find('date')\n",
    "    \n",
    "    if date_tag:\n",
    "        date_text = date_tag.get_text()\n",
    "        parsed_date = datetime.strptime(date_text, \"%d-%b-%Y %H:%M:%S.%f\")\n",
    "        elasticsearch_date = parsed_date.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "        return elasticsearch_date\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `extract_first_last_names` function to split the author name into the first and last names from the `author tag` in the data.\n",
    "\n",
    "* Use `extract_author` function to extract the author's name from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_last_names(author_tag):\n",
    "    # Extract author information from the given tag\n",
    "    author_info = author_tag.get_text() if author_tag else None\n",
    "    \n",
    "    # Check if author_info is not None before proceeding\n",
    "    if author_info:\n",
    "        # Define a case-insensitive regular expression pattern to extract the full author's name\n",
    "        pattern = re.compile(r'by (.*?),', re.IGNORECASE)\n",
    "\n",
    "        # Use the regular expression to find the match\n",
    "        match = re.search(pattern, author_info)\n",
    "\n",
    "        # Extract the full author's name\n",
    "        full_name = match.group(1) if match else None\n",
    "\n",
    "        # Split the full name into first and last names\n",
    "        if full_name:\n",
    "            names = full_name.split()\n",
    "            first_name = names[0] if names else None\n",
    "            last_name = names[-1] if len(names) > 1 else None\n",
    "            return first_name, last_name\n",
    "    return None, None\n",
    "\n",
    "def extract_authors(reuters_tag):\n",
    "    author_tag = reuters_tag.find('author')\n",
    "    return extract_first_last_names(author_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `extract_article_title` function to extract the article's title from the data `title` tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_title(reuters_tag):\n",
    "    title_tag = reuters_tag.find('title')\n",
    "    return title_tag.get_text() if title_tag else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `clean_text` function to remove unnecessary spaces and some special characters, like `\\n`.\n",
    "\n",
    "* Use `extract_content` function to extract the content of news content from the data `text` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw_text):\n",
    "    # Remove control characters and extra spaces\n",
    "    cleaned_text = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]+', ' ', raw_text)\n",
    "\n",
    "    # Remove leading and trailing whitespaces\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def extract_content(reuters_tag):\n",
    "    text_tag = reuters_tag.find('text')\n",
    "    raw_content = text_tag.get_text() if text_tag else None\n",
    "\n",
    "    # Use clean_text function to clean the extracted content\n",
    "    cleaned_content = clean_text(raw_content) if raw_content else None\n",
    "\n",
    "    return cleaned_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `extract_georeferences` to extract places and then send result of this function to `extract_geopoints` to get the location of place using\n",
    "\n",
    "* `geocode` method from `geopy` library to get latitude and longitude of this place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_georeferences(reuters_tag):\n",
    "    places_tag = reuters_tag.find('places')\n",
    "    return [place.get_text() for place in places_tag.find_all('d')] if places_tag else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `extract_temporal_expressions` to extract temporal expressions from the content of news using `spacy` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temporal_expressions(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ == 'DATE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `extract_geopoints` to convert place name to coordinates by get the location of place using `geocode` method from `geopy` library \n",
    "* to return latitude and longitude of this place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_geopoints(georeferences):\n",
    "    geolocator = Nominatim(user_agent=\"geo_app\")\n",
    "    geopoints = []\n",
    "    \n",
    "    for place_name in georeferences:\n",
    "        location = geolocator.geocode(place_name)\n",
    "        if location:\n",
    "            geopoints.append({'latitude': location.latitude, 'longitude': location.longitude})\n",
    "    return geopoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data indexing\n",
    "\n",
    "- Excluding documents that do not contain a `title` or `content`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_document(title, content, first_name,last_name, date, geopoints, temporal_expressions, georeferences):\n",
    "    try:\n",
    "        if title and content:\n",
    "            document = {\n",
    "                'title': title,\n",
    "                'content': content,\n",
    "                'date': date,\n",
    "                'authors':[{\"first_name\":first_name,\"last_name\":last_name}],\n",
    "                'geopoint': [{'lat': point['latitude'], 'lon': point['longitude']} for point in geopoints],\n",
    "                'temporalExpressions': temporal_expressions,\n",
    "                'georeferences': georeferences\n",
    "            }\n",
    "            \n",
    "            es.index(index='reuter_news_index', body=document)\n",
    "            print(f\"Document indexed successfully: {title}\")\n",
    "        else:\n",
    "            print(\"Skipping document due to missing required fields.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error indexing document: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sgm_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        \n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    reuters_tags = soup.find_all('reuters')\n",
    "    \n",
    "    for reuters_tag in reuters_tags:\n",
    "        date = extract_date(reuters_tag)\n",
    "        first_name, last_name = extract_authors(reuters_tag)\n",
    "        title = extract_article_title(reuters_tag)\n",
    "        file_content = extract_content(reuters_tag)\n",
    "        georeferences = extract_georeferences(reuters_tag)\n",
    "        temporal_expressions = extract_temporal_expressions(file_content)\n",
    "        \n",
    "        if georeferences:\n",
    "            geopoints = extract_geopoints(georeferences)\n",
    "        else:\n",
    "            geopoints = None\n",
    "            \n",
    "        index_document(title, file_content, first_name,last_name, date, geopoints, temporal_expressions, georeferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sgm_folder(folder_path):\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".sgm\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            read_sgm_file(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"./dummy data/\"\n",
    "process_sgm_folder(data_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
